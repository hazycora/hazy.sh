<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="./favicon.png" />
		<meta name="viewport" content="width=device-width" />
		<script src="/vendor/scroll-timeline-polyfill/scroll-timeline.min.js"></script>
		<script
			defer
			data-domain="hazy.sh"
			src="https://conscious.besties.house/js/script.js"
		></script>
		
		<link href="./_app/immutable/assets/0.CIAQ6ZAv.css" rel="stylesheet">
		<link href="./_app/immutable/assets/3.vwBCeKIY.css" rel="stylesheet">
		<link href="./_app/immutable/assets/Postscript.Cm_Q3krw.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.Di6TRgjC.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/entry.DBOXU0fs.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/runtime.HBcl4L12.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.D24iV_T_.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/store.CpKYbwEx.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/disclose-version.B6nF_LLs.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/props.CYQU2EHg.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/svelte-component.CRKHsW1q.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.BPJMMxbt.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/legacy.C4EjxfDb.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/3.B6CjOb20.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/dayjs.DornWgkT.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/lifecycle.BK3OBUpM.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/Postscript.C1bHBqvU.js"><!--[--><link rel="alternate" type="application/rss+xml" title="hazy's blog" href="/rss/"> <meta name="fediverse:creator" content="@h@besties.house"><!--]--><!--[--><!--[!--><!--]--> <meta property="og:site_name" content="hazy.sh"> <meta property="og:title" content="hazel's blog"> <meta name="twitter:title" content="hazel's blog"> <!--[--><meta name="description" content="hazel's blog for whatever"> <meta property="og:description" content="hazel's blog for whatever"> <meta name="twitter:description" content="hazel's blog for whatever"><!--]--> <!--[--><meta property="og:image" content="/avatar.webp"> <meta name="twitter:image" content="/avatar.webp"><!--]--> <!--[!--><meta name="twitter:card" content="summary"><!--]--><!--]--><title>hazel's blog</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><!----><!----><!----> <main class="svelte-lbl14x"><nav class="svelte-lbl14x"><a href="/" class="svelte-lbl14x">hazel cora</a> · <a href="/blog" class="svelte-lbl14x">blog</a></nav> <p class="svelte-lbl14x">Occasionally I write about the things I'm making, my experiences, and other
		things I find interesting.</p> <p class="svelte-lbl14x">The blog doesn't have any sort of schedule, so if you want to be sure to
		catch the next post you should <a href="https://social.besties.house/@h" class="svelte-lbl14x">follow me on the fediverse</a> or subscribe to <a href="/rss" class="svelte-lbl14x">the blog's RSS feed</a>!</p> <ul class="svelte-lbl14x"><!--[--><li class="svelte-lbl14x"><a href="/blog/webfishings-silly-filter" class="svelte-81na90 highlight"><!--[!--><!--]--> <span class="title svelte-81na90">Webfishing's bizarre profanity filter</span> <!--[--><p class="description svelte-81na90">Canadians are being CENSORED by the adorable furry fishing game</p><!--]--> <!--[--><span class="date svelte-81na90">Nov 11, 2024 · 5 months ago</span><!--]--></a><!----></li><li class="svelte-lbl14x"><a href="/blog/i-dont-care-what-the-osi-thinks" class="svelte-81na90 "><!--[!--><!--]--> <span class="title svelte-81na90">I kinda just don’t care what the OSI thinks</span> <!--[--><p class="description svelte-81na90">My projects are open for the greater good, not for military, policing, or bigots</p><!--]--> <!--[--><span class="date svelte-81na90">May 3, 2024 · a year ago</span><!--]--></a><!----></li><li class="svelte-lbl14x"><a href="/blog/twitter-summary-card-phishing" class="svelte-81na90 "><!--[--><img src="/blog/twitter-summary-card-phishing/card-example.png" alt="A Tweet displaying a summary card linking to discord.com" class="svelte-81na90"><!--]--> <span class="title svelte-81na90">Twitter's cards have a bug that makes phishing easy</span> <!--[--><p class="description svelte-81na90">Sites linked to on Twitter can pretend to be any other URL</p><!--]--> <!--[--><span class="date svelte-81na90">Jun 3, 2023 · 2 years ago</span><!--]--></a><!----></li><li class="svelte-lbl14x"><a href="/blog/bypassing-content-id" class="svelte-81na90 "><!--[!--><!--]--> <span class="title svelte-81na90">Bypassing YouTube Content-ID with flashing frames</span> <!--[--><p class="description svelte-81na90">Uploading copyrighted content to YouTube at the cost of epileptic seizures</p><!--]--> <!--[--><span class="date svelte-81na90">May 3, 2023 · 2 years ago</span><!--]--></a><!----></li><li class="svelte-lbl14x"><a href="/blog/download-youtube-subscriptions" class="svelte-81na90 "><!--[!--><!--]--> <span class="title svelte-81na90">Downloading YouTube subscriptions in CSV format with web scraping</span> <!--[--><p class="description svelte-81na90">Because Google Takeout takes too long</p><!--]--> <!--[--><span class="date svelte-81na90">May 23, 2022 · 3 years ago</span><!--]--></a><!----></li><!--]--></ul> <hr class="svelte-lbl14x"> <p class="postscript svelte-122ee6i">You can find me on the <a rel="me" href="https://social.besties.house/@h">fediverse</a> and my code on <a href="https://git.gay/h">git.gay</a>. If you'd like to
	support me, you can on <a href="https://ko-fi.com/@hazy">Ko-fi</a>, <a href="https://liberapay.com/hazy">Liberapay</a>, and <a href="https://github.com/sponsors/hazycora">GitHub Sponsors</a>.
	Thanks&lt;3</p><!----></main><!----><!----> <img src="/wallpaper.jpg" alt="" aria-hidden="true" class="wallpaper svelte-gnu9si"><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1s8nclx = {
						base: new URL(".", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					const data = [{"type":"data","data":null,"uses":{}},{"type":"data","data":{posts:[{frontmatter:{title:"Webfishing's bizarre profanity filter",description:"Canadians are being CENSORED by the adorable furry fishing game",date:"2024-11-11"},slug:"webfishings-silly-filter",path:"/blog/webfishings-silly-filter",date:new Date(1731283200000),body:"\n**Update:** The developer, lamedev, [knows of this](https://lethallava.land/notes/a0hatpsc0g)! I expect it'll be fixed in the next update. Also, my guess for the source of the list [was correct](https://lethallava.land/notes/a0hayv2l1n).\nThis post will remain as a curiosity :)\n\n---\n\nMy friend [Astra](https://astroorbis.com) was looking around a decompiled version of Webfishing--giving her access to some of\nthe source code and assets of the game--when she found the profanity list used by the recently-added chat filter. I've uploaded\nthis file as [a GitHub Gist](https://gist.github.com/hazycora/f7fde1be74022d196945577a1817d153).\n\nThe list is _almost_ entirely what you'd expect. Just a bunch of swears and slurs, exactly what I would want to be blocked if I\nhad a chat filter option turned on. But there are some strange outliers!\n\n\u003Cdiv style=\"font-size: 1.25rem\">\n\n**Before I continue:** The profanity list is very clearly sourced from some online list. I don't think the developer of Webfishing\nmade this list themselves or thoroughly checked it. These quirks are certainly a mistake. Do not interpret any of this as malice or bias.\n\n\u003C/div>\n\nHere are just a handful of words which are censored, and probably shouldn't be. This is almost certainly not everything, this is a pretty big\nlist to sift through!\n\n- arab\n- canadian\n- ethiopian\n- german\n- mexican\n- palestinian\n- israeli\n- jewish\n- latin\n- queer\n- gay\n- lesbian\n- bi (along with \"bi-sexual\", but curiously not \"bisexual\"?)\n- my personal favorite, \"women's\"\n\n![Trying to say 'I am an advocate for women's rights' results in the word 'women's' being censored.](/blog/webfishings-silly-filter/womens-rights.png)\n\nAnother interesting quirk is that some words in the list end with a space. This causes them to be completely ignored when filtering words.\n\n![The word bastard shown in the chat.](/blog/webfishings-silly-filter/bastard.png)\n\n## How'd that happen?\n\nSo, as I said in the disclaimer, I believe this list was certainly just copy-pasted from somewhere online. But where? I don't\nactually know where _exactly_ they found it, but I've got a pretty big lead. Searching Google for profanity lists, I stumbled upon\n[`bad-words.txt`](https://www.cs.cmu.edu/~biglou/resources/bad-words.txt), which includes many of these same words. It also includes many more words, though, so Webfishing's developer either tried to\nmanually sort through the list and didn't catch enough things, or they found a different version of this list.\n\nThe feature was added in the very last update, [Patch 1.09](https://store.steampowered.com/news/app/3146520/view/4467101633890746444), and I don't doubt it'll get fixed soon enough :)\n"},{frontmatter:{title:"I kinda just don’t care what the OSI thinks",description:"My projects are open for the greater good, not for military, policing, or bigots",date:"2024-05-03"},slug:"i-dont-care-what-the-osi-thinks",path:"/blog/i-dont-care-what-the-osi-thinks",date:new Date(1714694400000),body:"\nI’ve begun using the [Opinionated Queer License](https://oql.avris.it/) for many of my FOSS projects. It prohibits use of the projects by corporations which pay workers unfairly, prohibits use in military tech or for policing, and prohibits any bigoted use.\n\n\u003Cdiv class=\"dm\">\n\u003Cblockquote>\n\nA license prohibiting use by corporations isn’t FOSS!\n\n\u003C/blockquote>\n\u003Cdiv class=\"sent\">\n\nidc\n\n\u003C/div>\n\u003C/div>\n\nI’ve had this discussion a few times now with some fossbro types. Any project under the OQL isn’t considered open source by the Open Source Initiative because it doesn’t allow _everyone_ the same right to use the project. Instead, these projects are technically “source available”, the source code is visible to the public but use is only available under stricter terms.\n\n**But I don’t really care what the Open Source Initiative thinks?**\n\nI’ve made the source code of my projects available to make the world a better place, not to allow anything I’ve made to be used for evil. If you believe this makes my stuff not technically Open Source, I kinda just don’t care.\n\n“Open Source” is obviously a selling point for projects. I get more excited for a project when I see it’s open source, knowing I’d be allowed to use a project myself and extend it is great. This requirement that a project must be willing to be used for evil to be considered open source just serves to take the ability to promote projects as open source from people like myself.\n\nI’m going to continue to call my projects FOSS whether they’re under the MIT license, AGPL, or OQL. I don’t care that the OSI believes otherwise. The source is available for anyone, and can be used by whoever for anything that isn’t outright evil, and I think that’s all that matters. In the same way that some FOSS projects aren’t allowed to be used without sharing a copy of the license & modifications, my FOSS projects aren’t allowed to be used without having the decency to be an ally.\n"},{frontmatter:{image:"/blog/twitter-summary-card-phishing/card-example.png",imageAlt:"A Tweet displaying a summary card linking to discord.com",title:"Twitter's cards have a bug that makes phishing easy",description:"Sites linked to on Twitter can pretend to be any other URL",date:"2023-06-03"},slug:"twitter-summary-card-phishing",path:"/blog/twitter-summary-card-phishing",date:new Date(1685750400000),body:"\n(I'm not the first to find this problem. It's been known since 2019, I link to other articles and blog posts that came before me at the end.)\n\nWhen you link to something at the end of a Tweet, the link isn't displayed. Instead, you'll only see a \"summary card\" Twitter generates when a post is made. The domain name of the URL is displayed in these cards, so at face value they don't make phishing easier.\n\n![A Tweet displaying a summary card linking to discord.com](/blog/twitter-summary-card-phishing/card-example-domain-highlighted.png)\n\n## their implementation is Not Perfect\n\nIf a link redirects to another page, Twitter follows that redirect and displays the domain name of the destination instead of the original URL. This is fine, it means the cards will reflect the same page users will see.\nBut the user isn't sent directly to the destination- When someone clicks one of these cards, they go to the original URL. For typical redirect links, that's okay, because the redirect Twitter experiences will we the same as the user's redirect. But what if a malicious site serves a different redirect to users than to Twitter?\n\nWhen Twitter fetches the page content in order to generate these cards, they set the HTTP \"User-Agent\" header to begin with \"Twitterbot/\", so it can be detected on our website that the request was for Twitter. Setting custom User-Agents for things like this is normal, it means sites can customise the text of embeds for specific sites if necessary or block Twitter from making these requests if desired, but for malicious sites it also allows us to exploit this issue reliably.\n\nHere's an example of an ExpressJS route that detects Twitter and redirects to discord.com, while redirecting to example.com for everyone else:\n\n```js\napp.get('/', (req, res, next) => {\n\tconst detectedTwitter = req.headers['user-agent']\n\t\t.toLowerCase()\n\t\t.startsWith('Twitterbot/')\n\n\tif (detectedTwitter) {\n\t\t// redirect to the page to be displayed on Twitter\n\t\tres.redirect('https://discord.com')\n\t\treturn\n\t}\n\n\t// handle request for other users\n\t// malicious sites could serve a phishing site right here\n\tres.redirect('https://example.com')\n})\n```\n\n![An animated GIF showing a Tweet that looks to contain a link to Discord, but sends users to example.com instead](/blog/twitter-summary-card-phishing/demo.gif)\n\n## this issue is old\n\nTwitter has had this issue for years, and I'm by no means the first to notice it. It's [been known since 2019](https://shkspr.mobi/blog/2019/03/scammers-abusing-twitter-cards-via-redirects/) at the latest. My friend [Lexi](https://twitter.com/1lexxi) found the same issue a couple years ago, and she told me it reported this to Twitter through HackerOne, but the exploit still hasn't been fixed.\n\n\u003Cdiv class=\"dm\">\n\u003Cblockquote>\n\ntwitters bbp is a joke, fixing things is not their strong suit\n\n\u003C/blockquote>\n\u003Cdiv>\n\nLmao\n\n\u003C/div>\n\u003C/div>\n\nBleepingComputer [wrote on this in 2019](https://www.bleepingcomputer.com/news/security/twitter-can-be-tricked-into-showing-misleading-embedded-links/) and contacted Twitter about it, to no response.\n\n> BleepingComputer reached out to Twitter for a statement about this problem and if it would be fixed in the near future but received no reply at the time of publishing.\n\nHopefully Twitter realises the severity of this problem. Be careful when clicking links- the suggestion to check URLs before clicking is a good one, but it's not infallible!\n"},{frontmatter:{title:"Bypassing YouTube Content-ID with flashing frames",description:"Uploading copyrighted content to YouTube at the cost of epileptic seizures",date:"2023-05-03",contentWarning:"Flashing lights"},slug:"bypassing-content-id",path:"/blog/bypassing-content-id",date:new Date(1683072000000),body:"\nAnyone who archives media to YouTube knows the struggle that is working around YouTube's Content ID system. It's the system which detects copyrighted material in order to allow media companies to earn advertising revenue from videos which use their content, but it also allows these companies to outright block videos from many parts of the world. Of course, YouTube couldn’t exist without Content ID, but that doesn’t make it any less frustrating.\n\n## What if there was a better way?\n\nYouTube's Content ID is pretty sophisticated. People have tried many, many things to get around it. Various visual effects, like hue-shifting, vignette filters, adding massive coloured borders surrounding the video, and many more have already been tried numerous times. These tricks work _sometimes_, but YouTube's Content ID has only gotten better and better at working around these tricks. Any solution needs to make the copyrighted content completely invisible to Content ID, but this very often also means obscuring it for actual human viewers.\n\nBut I've had an idea in the back of my mind since I saw a video that appeared in my recommendations a couple months ago. It's a [Bad Apple video](https://en.wikipedia.org/wiki/Bad_Apple!!#Use_as_a_graphical_and_audio_test) which can only be viewed in HD. How this works is that 60fps YouTube videos don't actually play at 60fps unless you view them at 720p or higher, so you can replace every other frame with an entirely black frame and the video will be invisible if you're not watching with an HD quality setting.\n\n@embed:https://www.youtube.com/watch?v=lwi7ofgZ8ME\n\nWhat if Content ID doesn't scan the HD 60fps versions of videos? Would it then be completely oblivious to half of the frames of the video?\n\n**This isn't a good idea**. It would make it impossible to watch on low resolutions, harder to compress, makes the video uncomfortable to watch and could even cause watching the video to _**induce seizures**_ in people with photosensitive epilepsy. This is not a good solution to the problem.\n\n_But would it work though?_\n\n## `ffmpeg` is a challenge\n\nI didn't want to extract all the frames and then swap every other one with a black image, I thought that would probably take too long. Instead I looked into `ffmpeg`'s `geq` video filter. It allows you to make an expression which is ran per-pixel based on a few variables. `ffmpeg` also has `if` and `mod` functions, so this seemed like it would be a fairly easy task.\n\n```sh\nffmpeg -i input.mp4 -y -vf \"fps=60,geq=if(mod(N\\,2)\\,p(X\\,Y))\" -preset ultrafast output.mp4\n```\n\n![an ffplay window showing a dark green frame](/blog/bypassing-content-id/green-frame.png)\n\nOkay, so this doesn't work how I thought it would. I assumed the color value `0` would be black, but it's instead some dark muddy green colour? I think this is because `ffmpeg` is using the [YUV color model](https://en.wikipedia.org/wiki/YUV). I'm not exactly sure how I should get it to use the color black for these frames using a YUV color value, but luckily the `geq` filter allows me to set expressions for the r, g, and b values of each pixel instead:\n\n```sh\nffmpeg -i input.mp4 -y -vf \"fps=60,geq=r=if(mod(N\\,2)\\,p(X\\,Y)):g=if(mod(N\\,2)\\,p(X\\,Y)):b=if(mod(N\\,2)\\,p(X\\,Y))\" -preset ultrafast output.mp4\n```\n\nTo be clear, this command sucks. It's evaluating the expression on every single pixel in every single frame. I'm positive there are wayyyy better ways to do this, but it was taking me a while to figure this out to begin with so I'm willing to just go with this command for now. If anyone's figured out a better way to do this in `ffmpeg`, please do message me!\n\nI'm getting 0.5x speeds out of this at best on my PC. To be fair, this PC's specs are abysmal, so that wasn't doing it any favours.\n\n## does this actually work\n\nI sent the script off to my friend [Aria](https://tacohitbox.com), who spends An Excessive Amount Of Time archiving media to various places including YouTube.\n\n![tacohitbox: NO FUCKING SHOT, DUDE THE CONSTANT FLICKERING ONE WORKED; hazy: HAHAHAA, send the fucking link; tacohitbox: THIS IS SO FUCKING FUNNY](/blog/bypassing-content-id/it-works.png)\n\nReally?? This is so stupid. This is completely useless. No way this actually works. I'm running the command on a video real quick to try it out for myself.\n\n![Screenshot of YouTube Studio, showing an audio claim for Family Guy - Life of Brian, blocking the video in all territories](/blog/bypassing-content-id/no-family-guy-funnies.png)\n\nNooooo. I forgot about this. Some shows also do Content ID for the audio of episodes. Even if the video itself is completely undetectable, the video may still get claimed if it contains audio from something. So I'm going to have to do some more alterations to this video to make it get through Content ID.\n\nI think, for this demo, I'll just put some music over the video. Music gets Content ID too, but music labels tend to not block videos which contain music, instead just taking ad revenue. I'm completely willing to give some music label all the ad revenue for this video so long as it's visible.\n\n![Screenshot of the Kdenlive video editor with an audio track filled with Bladee songs](/blog/bypassing-content-id/kdenlive-drained.png)\n\nI'm so sorry, but The Family Guy Funny Moments will have to become drained.\n\n## family guy funny moments (drained edition) (WARNING: FLASHING LIGHTS)\n\n\u003Cp>\n\t\u003Ciframe src=\"https://www.youtube.com/embed/XrvbAuk42LE?start=450\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen style=\"width: 100%; aspect-ratio: 16 / 9; border-radius: 0.75rem\">\u003C/iframe>\n\u003C/p>\n\nthis has got to be the worst thing i've ever made\n\nDrain gang music added to prevent audio claims. [Here's the playlist, lol](https://open.spotify.com/playlist/6vi42OTDujqUXTfyd0DYNZ). Hard-coded subtitles added to make this at least a little bit watchable. All the music I added has gotten detected by Content ID _with zero restrictions_, and the Family Guy episode itself hasn't gotten claimed at all. This video can be watched globally. If I was in the YouTube Partner Program I could even earn advertising revenue from this video.\n\nAnd, in case anyone asks, yes, even with the drain gang music this video gets claimed if I don't use the strobe effect:\n\n![Screenshot showing 'family guy drainer nostrobe' blocked in YouTube Studio](/blog/bypassing-content-id/blocked-drainer-nostrobe-upload.png)\n\nWith the strobe effect, the Family Guy Funny Moments are undetected by Content ID.\n\n![Screenshot showing the strobe effect video hasn't gotten blocked in YouTube Studio](/blog/bypassing-content-id/no-block.png)\n\n## what have we learned\n\nYou can get around YouTube Content ID! As long as you're willing to ignore these caveats:\n\n- it's uncomfortable to watch\n- causes massive video file size\n- requires exporting at 720p or higher at 60fps\n- audio can still get claimed so you'll have to distort that, cover it up with something else, or mute it entirely\n- _**could cause seizures**_\n\nPlease, please don't start actually using this to evade Content ID. This isn't a practical solution. Even if this idea were to inspire a more practical Content-ID-evading solution, always remember YouTube will simply patch it if it ever becomes widespread.\n"},{frontmatter:{title:"Downloading YouTube subscriptions in CSV format with web scraping",description:"Because Google Takeout takes too long",date:"2022-05-23"},slug:"download-youtube-subscriptions",path:"/blog/download-youtube-subscriptions",date:new Date(1653264000000),body:"\nHey! This was posted [on my dev.to profile back in 2022](https://dev.to/hazy/downloading-your-youtube-subscriptions-in-csv-format-because-google-takeout-takes-too-long-5ca1). Apparently it was shared on Reddit, and for archival's sake I'm reposting it here, two years later! The rest of the article is pretty much just as I wrote it in 2022. Excuse poor writing and atrocious code formatting, it's been a bit!\n\n---\n\nI wanted to import my YouTube subscriptions into the [open-source Android YouTube client NewPipe](https://newpipe.net/). The normal way to do that is to export the subscriptions from Google Takeout, a service Google provides to allow you to retrieve data about your account. NewPipe kindly explains the process:\n\n![Screenshot of the NewPipe app's explanation on how to use Google Takeout.](/blog/download-youtube-subscriptions/instructions.webp)\n\n_Excuse the comic sans, it's my favorite font._\n\nNewPipe's instructions are as follows:\n\n> Import YouTube subscriptions from Google takeout:\n>\n> 1. Go to this URL: https://takeout.google.com/takeout/custom/youtube\n> 2. Log in when asked\n> 3. Click on \"All data included\", then on \"Deselect all\", then select only \"subscriptions\" and click \"OK\"\n> 4. Click on \"Next step\" and then on \"Create export\"\n> 5. Click on the \"Download\" button after it appears\n> 6. Click on IMPORT FILE below and select the downloaded .zip file\n> 7. [If the .zip import fails] Extract the .csv file (usually under \"YouTube and YouTube Music/subscriptions/subscriptions.csv\"), click on IMPORT FILE below and select the extracted csv file\n\nWhat they neglect to mention is that Google Takeout can take many hours to complete.\n\nI tried using Google Takeout, but after an hour of waiting, I decided I'd try something else. I would scrape the list of channels I'm subscribed to, and I'd save that list as a CSV file I can import into NewPipe.\n\n## Finding out how the Google Takeout CSV is formatted\n\nIn order to make my own file that NewPipe would accept as though it were a Google Takeout CSV, I had to find out the format Google Takeout uses.\n\nI found [the PR](https://github.com/TeamNewPipe/NewPipeExtractor/pull/709/commits/94a29fd63ff6bb0c1805c44ef5ebf4d915427454) that added the Google Takeout importing support. Inside that commit was a description of the file.\n\n```java\n// Expected format of CSV file:\n//      Channel Id,Channel Url,Channel Title\n//      UC1JTQBa5QxZCpXrFSkMxmPw,http://www.youtube.com/channel/UC1JTQBa5QxZCpXrFSkMxmPw,Raycevick\n//      UCFl7yKfcRcFmIUbKeCA-SJQ,http://www.youtube.com/channel/UCFl7yKfcRcFmIUbKeCA-SJQ,Joji\n//\n// Notes:\n//      It's always 3 columns\n//      The first line is always a header\n//      Header names are different based on the locale\n//      Fortunately the data is always the same order no matter what locale\n```\n\nThis was simple enough. Now I needed to find out how to get a list of the Channel ID and Channel Title of each of the channels I'm subscribed to.\n\n## YouTube scraping\n\nI found a page on YouTube [that lists all the channels you're subscribed to](https://www.youtube.com/feed/channels). I got to looking at how this page worked, and realized that the page stores data inside a variable called ytInitialData. This variable stores the list of the channels you're subscribed to, as well as some other data. YouTube paginates the list, though, so the variable won't always have everything right off the bat. YouTube loads more of the list whenever you scroll to the bottom of the page, though, which means I can just automate the scrolling.\n\nAnother bit of data included in the ytInitialData variable is the API token required to load the rest of the list. And when the list is fully loaded, the token is removed from ytInitialData.\n\nThis means I can check if that token exists in order to know whether to keep scrolling down.\n\nI wrote a script to scroll to the bottom of the page by checking the height of the container `\u003Cdiv>` and then scrolling with the function `window.scrollTo`.\n\nI then wrote a script to get the Channel IDs and Channel Titles from this list. I would also need the Channel URL, but this was as easy as adding the channel ID after the string `\"http://www.youtube.com/channel/\"`. It then joined the data of all the channels one by one. Finally, it would log the data to the console.\n\nI combined these scripts and ran them together. It worked.\n\nTo make it just a bit more convenient, I made another script which made a `\u003Cdiv>` to put the CSV data in. This `\u003Cdiv>` would use `position: fixed` to cover the screen.\n\nTo make it even easier to use, I decided to make a download button to save the text as a file so you wouldn't need to copy-paste it yourself.\n\n## The script\n\nHere's my code:\n\n```js\nfunction getLast() {\n\treturn ytInitialData.contents.twoColumnBrowseResultsRenderer.tabs[0].tabRenderer.content.sectionListRenderer.contents.slice(\n\t\t-1\n\t)[0]\n}\nfunction canContinue() {\n\treturn getLast().continuationItemRenderer != null\n}\n;(async () => {\n\twhile (canContinue()) {\n\t\tlet current =\n\t\t\tgetLast().continuationItemRenderer.continuationEndpoint\n\t\t\t\t.continuationCommand.token\n\t\tscrollTo(0, document.getElementById('primary').scrollHeight)\n\t\twhile (\n\t\t\tcanContinue() &&\n\t\t\tcurrent ==\n\t\t\t\tgetLast().continuationItemRenderer.continuationEndpoint\n\t\t\t\t\t.continuationCommand.token\n\t\t) {\n\t\t\tawait new Promise(r => setTimeout(r, 100))\n\t\t}\n\t}\n\tscrollTo(0, 0)\n\tlet floatDiv = document.createElement('div')\n\tlet preText = document.createElement('pre')\n\tfloatDiv.setAttribute(\n\t\t'style',\n\t\t`position: fixed;\n\t\tbackground: #0f0f0f;\n\t\tz-index: 100000;\n\t\tinset: 2rem;\n\t\toverflow: auto;\n\t\tfont-size: 2rem;\n\t\twhite-space: pre;\n\t\tcolor: white;\n\t\tpadding: 1rem;`\n\t)\n\tlet csvText =\n\t\t'Channel Id,Channel Url,Channel Title\\n' +\n\t\tytInitialData.contents.twoColumnBrowseResultsRenderer.tabs[0].tabRenderer.content.sectionListRenderer.contents\n\t\t\t.map(e => {\n\t\t\t\tif (!e.itemSectionRenderer) return\n\t\t\t\treturn e.itemSectionRenderer.contents[0].shelfRenderer.content\n\t\t\t\t\t.expandedShelfContentsRenderer.items\n\t\t\t})\n\t\t\t.flat()\n\t\t\t.map(e => {\n\t\t\t\tif (e && e.channelRenderer)\n\t\t\t\t\treturn `${e.channelRenderer.channelId},http://www.youtube.com/channel/${e.channelRenderer.channelId},${e.channelRenderer.title.simpleText}`\n\t\t\t\treturn ''\n\t\t\t})\n\t\t\t.join('\\n')\n\tpreText.innerText = csvText\n\tlet downloadLink = document.createElement('a')\n\tdownloadLink.innerText = 'Download CSV'\n\tdownloadLink.setAttribute('target', '_blank')\n\tdownloadLink.setAttribute(\n\t\t'style',\n\t\t`color: #bf3838;\n\t\tfont-weight: bold;\n\t\tmargin-bottom: 1rem;\n\t\tdisplay: block;\n\t\tpadding: 1rem;\n\t\tborder-radius: 0.5rem;\n\t\tborder: 2px solid #bf3838;\n\t\twidth: fit-content;\n\t\ttext-decoration: none;`\n\t)\n\tvar t = new Blob([csvText], { type: 'text/plain' })\n\tdownloadLink.href = window.URL.createObjectURL(t)\n\tfloatDiv.appendChild(downloadLink)\n\tfloatDiv.appendChild(preText)\n\tdocument.body.appendChild(floatDiv)\n})()\n```\n\nYou can run this in DevTools at [youtube.com/feed/channels](https://www.youtube.com/feed/channels). You can then save the file by clicking \"Download CSV\".\n\n![A red button reading 'Download CSV', above a list of YouTube Channel IDs and Channel URLs.](/blog/download-youtube-subscriptions/screenshot.jpeg)\n\n## Conclusion\n\nBy the time I had finished writing the script, Google Takeout still hadn't sent me a download of my subscriptions list. I eventually received this download hours later, but by that point I had already imported my own list into NewPipe and no longer needed theirs.\n\nMy guess is that Google Takeout runs on a queue, sending one person their data and then the next, rather than working on each request immediately.\n\nI hope you find my script useful. If you make any improvements, you can let me know in the comments of my [GitHub gist](https://gist.github.com/hazycora/bc41e673aff4c9c7846d80e145574285) about this script.\n"}]},"uses":{}}];

					Promise.all([
						import("./_app/immutable/entry/start.Di6TRgjC.js"),
						import("./_app/immutable/entry/app.D24iV_T_.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
